{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AST (Audio Spectrogram Transformer) on ESC-50  \n",
    "このノートブックは、論文 **AST: Audio Spectrogram Transformer** (Gong et al., 2021) を **ESC-50** で試すための “ノンストップ” 実験ノートです。  \n",
    "\n",
    "やること：\n",
    "1. ESC-50 を読み込んで train/val/test に分割（既存 `src` を再利用）\n",
    "2. `MIT/ast-finetuned-audioset-10-10-0.4593` をベースに fine-tune（分類 head を 50 クラスに差し替え）\n",
    "3. test 推論 → `inference_summary.csv` 出力 → accuracy / macro-F1 / weighted-F1\n",
    "4. **入力特徴（ASTFeatureExtractorの mel-fbank）** と **エンコード後特徴（pooler_output）** を UMAP で比較し、  \n",
    "   「クラス分離が学習で良くなったか」を目で確認\n",
    "\n",
    "注意：\n",
    "- このノートは **editable install (`pip install -e .`)** 済みを想定しています。未実施なら最初に実行してください。  \n",
    "- torchaudio の `torchcodec` 依存でハマらないよう、wav 読み込みは **soundfile** を使います（HF docs でもOKと明記）。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: 3.12.3 (tags/v3.12.3:f6650f9, Apr  9 2024, 14:05:25) [MSC v.1938 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# If you haven't done it yet (project rootで):\n",
    "# pip install -e .\n",
    "\n",
    "import sys, os\n",
    "print(\"python:\", sys.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 依存関係（必要ならインストール）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要に応じて。すでに入っているならスキップでOK。\n",
    "# !pip install -U transformers umap-learn soundfile scipy scikit-learn pyyaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) プロジェクトルート探索 + config 読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT = C:\\Users\\hirok\\code\\02_sound\\esc50_cnn\n",
      "Loaded config: C:\\Users\\hirok\\code\\02_sound\\esc50_cnn\\config\\default.yaml\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def find_project_root(start: Path | None = None) -> Path:\n",
    "    start = (start or Path.cwd()).resolve()\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / \"config\").exists() and (p / \"src\").exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"Project root not found. Open this notebook somewhere under the project folder.\")\n",
    "\n",
    "ROOT = find_project_root()\n",
    "print(\"ROOT =\", ROOT)\n",
    "\n",
    "CFG_PATH = ROOT / \"config\" / \"default.yaml\"\n",
    "cfg = yaml.safe_load(CFG_PATH.read_text(encoding=\"utf-8\"))\n",
    "print(\"Loaded config:\", CFG_PATH)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) ESC-50 metadata / split（既存 `src` を再利用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta rows: 2000 num_classes: 50\n",
      "{'train': 1200, 'val': 400, 'test': 400}\n"
     ]
    }
   ],
   "source": [
    "from src.data.metadata import load_esc50_metadata\n",
    "from src.data.split import SplitConfig, make_splits\n",
    "\n",
    "meta_csv = (ROOT / Path(cfg[\"paths\"][\"meta_csv\"])).resolve()\n",
    "audio_dir = (ROOT / Path(cfg[\"paths\"][\"audio_dir\"])).resolve()\n",
    "\n",
    "df = load_esc50_metadata(meta_csv)\n",
    "print(\"meta rows:\", len(df), \"num_classes:\", df[\"target\"].nunique())\n",
    "assert df[\"target\"].nunique() == 50\n",
    "\n",
    "s_cfg = cfg[\"split\"]\n",
    "split_cfg = SplitConfig(\n",
    "    method=s_cfg[\"method\"],\n",
    "    train_folds=list(s_cfg.get(\"train_folds\", [])),\n",
    "    val_folds=list(s_cfg.get(\"val_folds\", [])),\n",
    "    test_folds=list(s_cfg.get(\"test_folds\", [])),\n",
    "    kfold_num_folds=int(s_cfg.get(\"kfold\", {}).get(\"num_folds\", 5)),\n",
    "    kfold_test_fold=int(s_cfg.get(\"kfold\", {}).get(\"test_fold\", 1)),\n",
    "    kfold_val_fold=int(s_cfg.get(\"kfold\", {}).get(\"val_fold\", 2)),\n",
    "    random_seed=int(cfg[\"project\"][\"seed\"]),\n",
    ")\n",
    "splits = make_splits(df, split_cfg)\n",
    "print({k: len(v) for k, v in splits.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) AST のセットアップ（FeatureExtractor / Model）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hirok\\venv\\py312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([50]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([50, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded: MIT/ast-finetuned-audioset-10-10-0.4593\n",
      "model num_labels: 50\n",
      "feature_extractor sampling_rate: 16000\n",
      "feature_extractor max_length: 1024 num_mel_bins: 128\n"
     ]
    }
   ],
   "source": [
    "import time, math\n",
    "import soundfile as sf\n",
    "from scipy.signal import resample_poly\n",
    "\n",
    "from transformers import ASTFeatureExtractor, ASTForAudioClassification\n",
    "\n",
    "PRETRAINED = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(PRETRAINED)\n",
    "\n",
    "# label mapping (ESC-50)\n",
    "target_to_cat = df.sort_values(\"target\").drop_duplicates(\"target\").set_index(\"target\")[\"category\"].to_dict()\n",
    "id2label = {int(t): str(target_to_cat[int(t)]) for t in range(50)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model = ASTForAudioClassification.from_pretrained(\n",
    "    PRETRAINED,\n",
    "    num_labels=50,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,  # headがAudioSet(527)→ESC-50(50)でサイズ不一致になるため\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "print(\"loaded:\", PRETRAINED)\n",
    "print(\"model num_labels:\", model.config.num_labels)\n",
    "print(\"feature_extractor sampling_rate:\", feature_extractor.sampling_rate)\n",
    "print(\"feature_extractor max_length:\", feature_extractor.max_length, \"num_mel_bins:\", feature_extractor.num_mel_bins)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Dataset / DataLoader（soundfile + resample + ASTFeatureExtractor）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch input: torch.Size([32, 1024, 128]) labels: torch.Size([32]) example fn: 3-100018-A-18.wav\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def load_wav_mono_resampled(path: Path, target_sr: int) -> np.ndarray:\n",
    "    x, sr = sf.read(str(path), always_2d=False)\n",
    "    if x.ndim == 2:\n",
    "        x = x.mean(axis=1)  # stereo -> mono\n",
    "    x = x.astype(np.float32)\n",
    "    if sr != target_sr:\n",
    "        # resample_poly: good quality & fast\n",
    "        g = math.gcd(sr, target_sr)\n",
    "        up = target_sr // g\n",
    "        down = sr // g\n",
    "        x = resample_poly(x, up=up, down=down).astype(np.float32)\n",
    "    return x\n",
    "\n",
    "class Esc50AstDataset(Dataset):\n",
    "    def __init__(self, df_sub: pd.DataFrame, audio_dir: Path, fe: ASTFeatureExtractor):\n",
    "        self.df = df_sub.reset_index(drop=True)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.fe = fe\n",
    "        self.sr = int(fe.sampling_rate)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        fn = row[\"filename\"]\n",
    "        y = int(row[\"target\"])\n",
    "        wav_path = self.audio_dir / fn\n",
    "        x = load_wav_mono_resampled(wav_path, self.sr)\n",
    "        return {\"audio\": x, \"label\": y, \"filename\": fn}\n",
    "\n",
    "def collate_ast(batch, fe: ASTFeatureExtractor):\n",
    "    audios = [b[\"audio\"] for b in batch]\n",
    "    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n",
    "    fns = [b[\"filename\"] for b in batch]\n",
    "\n",
    "    feats = fe(\n",
    "        audios,\n",
    "        sampling_rate=fe.sampling_rate,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    # feats[\"input_values\"]: [B, max_length, num_mel_bins]\n",
    "    return feats[\"input_values\"], labels, fns\n",
    "\n",
    "dl_cfg = cfg[\"dataloader\"]\n",
    "batch_size = int(dl_cfg.get(\"batch_size\", 32))\n",
    "num_workers = int(dl_cfg.get(\"num_workers\", 0))\n",
    "# Recommended default on Windows notebooks to avoid multiprocessing/pickling issues.\n",
    "if os.name == \"nt\":\n",
    "    num_workers = 0\n",
    "\n",
    "ds_train = Esc50AstDataset(splits[\"train\"], audio_dir, feature_extractor)\n",
    "ds_val   = Esc50AstDataset(splits[\"val\"], audio_dir, feature_extractor)\n",
    "ds_test  = Esc50AstDataset(splits[\"test\"], audio_dir, feature_extractor)\n",
    "\n",
    "def collate_ast_fixed(batch):\n",
    "    # NOTE (Windows): DataLoader with num_workers>0 requires picklable functions.\n",
    "    # Avoid lambda to prevent PicklingError.\n",
    "    return collate_ast(batch, feature_extractor)\n",
    "\n",
    "train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True,  num_workers=num_workers,\n",
    "                          collate_fn=collate_ast_fixed)\n",
    "val_loader   = DataLoader(ds_val,   batch_size=batch_size, shuffle=False, num_workers=num_workers,\n",
    "                          collate_fn=collate_ast_fixed)\n",
    "test_loader  = DataLoader(ds_test,  batch_size=batch_size, shuffle=False, num_workers=num_workers,\n",
    "                          collate_fn=collate_ast_fixed)\n",
    "\n",
    "x0, y0, f0 = next(iter(train_loader))\n",
    "print(\"batch input:\", x0.shape, \"labels:\", y0.shape, \"example fn:\", f0[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) 学習ループ（fine-tune → best checkpoint 保存）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_dir: C:\\Users\\hirok\\code\\02_sound\\esc50_cnn\\reports\\ast_20260118-100246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=2.7173 | val_loss=1.7254 | val_macro_f1=0.8365 | val_acc=0.8475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 2:  45%|████▍     | 17/38 [18:24<22:41, 64.83s/it, loss=1.21]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def run_eval(model, loader, device):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y, _ in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model(input_values=x, labels=y)\n",
    "            loss = out.loss\n",
    "            logits = out.logits\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "\n",
    "            total_loss += float(loss.item()) * y.size(0)\n",
    "            n += y.size(0)\n",
    "            ys.append(y.detach().cpu().numpy())\n",
    "            ps.append(pred.detach().cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(ys)\n",
    "    y_pred = np.concatenate(ps)\n",
    "    return {\n",
    "        \"loss\": total_loss / max(n, 1),\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"macro_f1\": float(f1_score(y_true, y_pred, average=\"macro\", zero_division=0)),\n",
    "        \"weighted_f1\": float(f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
    "    }\n",
    "\n",
    "def train_ast(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    *,\n",
    "    epochs: int = 10,\n",
    "    lr: float = 1e-5,  # ASTは低め推奨（HF docsでも注意）\n",
    "    weight_decay: float = 1e-4,\n",
    "    grad_clip: float = 1.0,\n",
    "    mixed_precision: bool = True,\n",
    "    out_dir: Path | None = None,\n",
    "):\n",
    "    out_dir = out_dir or (ROOT / \"reports\" / f\"ast_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"out_dir:\", out_dir)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=(mixed_precision and device.type == \"cuda\"))\n",
    "\n",
    "    best = {\"macro_f1\": -1.0, \"epoch\": -1}\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        pbar = tqdm(train_loader, desc=f\"train epoch {epoch}\", leave=False)\n",
    "        running = 0.0\n",
    "        n = 0\n",
    "\n",
    "        for x, y, _ in pbar:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.amp.autocast(\"cuda\", enabled=(mixed_precision and device.type == \"cuda\")):\n",
    "                out = model(input_values=x, labels=y)\n",
    "                loss = out.loss\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running += float(loss.item()) * y.size(0)\n",
    "            n += y.size(0)\n",
    "            pbar.set_postfix(loss=running / max(n, 1))\n",
    "\n",
    "        val_stats = run_eval(model, val_loader, device)\n",
    "        row = {\"epoch\": epoch, \"train_loss\": running / max(n, 1), **{f\"val_{k}\": v for k, v in val_stats.items()}}\n",
    "        history.append(row)\n",
    "        print(f\"Epoch {epoch}: train_loss={row['train_loss']:.4f} | val_loss={val_stats['loss']:.4f} | val_macro_f1={val_stats['macro_f1']:.4f} | val_acc={val_stats['accuracy']:.4f}\")\n",
    "\n",
    "        if val_stats[\"macro_f1\"] > best[\"macro_f1\"]:\n",
    "            best = {\"macro_f1\": val_stats[\"macro_f1\"], \"epoch\": epoch}\n",
    "            ckpt = out_dir / \"best.pt\"\n",
    "            torch.save({\"model_state_dict\": model.state_dict(), \"epoch\": epoch, \"val\": val_stats}, ckpt)\n",
    "\n",
    "    torch.save({\"model_state_dict\": model.state_dict(), \"epoch\": epochs}, out_dir / \"last.pt\")\n",
    "    pd.DataFrame(history).to_csv(out_dir / \"train_log.csv\", index=False)\n",
    "    print(\"best:\", best)\n",
    "    return out_dir\n",
    "\n",
    "# ---- Train ----\n",
    "EPOCHS = int(cfg[\"train\"].get(\"epochs\", 10))\n",
    "LR = float(cfg[\"train\"].get(\"lr\", 1e-5))\n",
    "out_dir = train_ast(model, train_loader, val_loader, device, epochs=EPOCHS, lr=LR)\n",
    "out_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Test 推論 → inference_summary.csv → 指標算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def load_checkpoint(model, ckpt_path: Path, device):\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    return ckpt\n",
    "\n",
    "ckpt_best = out_dir / \"best.pt\"\n",
    "_ = load_checkpoint(model, ckpt_best, device)\n",
    "print(\"Loaded best:\", ckpt_best)\n",
    "\n",
    "model.eval()\n",
    "rows = []\n",
    "with torch.no_grad():\n",
    "    for x, y, fns in tqdm(test_loader, desc=\"infer\", leave=False):\n",
    "        x = x.to(device)\n",
    "        logits = model(input_values=x).logits\n",
    "        prob = torch.softmax(logits, dim=1).detach().cpu().numpy()\n",
    "        pred = prob.argmax(axis=1)\n",
    "\n",
    "        y_np = y.numpy()\n",
    "        for i, fn in enumerate(fns):\n",
    "            row = {\n",
    "                \"filename\": fn,\n",
    "                \"y_true\": int(y_np[i]),\n",
    "                \"y_pred\": int(pred[i]),\n",
    "                \"y_prob\": float(prob[i, pred[i]]),\n",
    "            }\n",
    "            for c in range(prob.shape[1]):\n",
    "                row[f\"p_{c:02d}\"] = float(prob[i, c])\n",
    "            rows.append(row)\n",
    "\n",
    "summary = pd.DataFrame(rows)\n",
    "summary_path = out_dir / \"inference_summary.csv\"\n",
    "summary.to_csv(summary_path, index=False)\n",
    "print(\"Wrote:\", summary_path)\n",
    "\n",
    "y_true = summary[\"y_true\"].to_numpy()\n",
    "y_pred = summary[\"y_pred\"].to_numpy()\n",
    "metrics = {\n",
    "    \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "    \"macro_f1\": float(f1_score(y_true, y_pred, average=\"macro\", zero_division=0)),\n",
    "    \"weighted_f1\": float(f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
    "}\n",
    "print(\"Metrics:\", metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) UMAP: 入力特徴 vs エンコード後特徴（pooler_output）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def batch_input_features(loader, max_items: int | None = None):\n",
    "    # ASTFeatureExtractorの出力 (input_values) から、簡単な要約ベクトルを作る\n",
    "    # 例: mel binごとに time平均と標準偏差を連結 → 256次元\n",
    "    Xs, ys, fns = [], [], []\n",
    "    seen = 0\n",
    "    for x, y, fn in tqdm(loader, desc=\"collect input features\", leave=False):\n",
    "        x_np = x.numpy()            # [B, T, M]\n",
    "        mu = x_np.mean(axis=1)      # [B, M]\n",
    "        sd = x_np.std(axis=1)       # [B, M]\n",
    "        feat = np.concatenate([mu, sd], axis=1)  # [B, 2M]\n",
    "        Xs.append(feat)\n",
    "        ys.append(y.numpy())\n",
    "        fns.extend(fn)\n",
    "        seen += x_np.shape[0]\n",
    "        if max_items and seen >= max_items:\n",
    "            break\n",
    "    return np.concatenate(Xs), np.concatenate(ys), np.array(fns)\n",
    "\n",
    "def batch_encoded_features(model, loader, device, max_items: int | None = None):\n",
    "    # AST encoder の pooler_output を集める（[B, hidden]）\n",
    "    model.eval()\n",
    "    Xs, ys, fns = [], [], []\n",
    "    seen = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y, fn in tqdm(loader, desc=\"collect encoded features\", leave=False):\n",
    "            x = x.to(device)\n",
    "            enc = model.ast(input_values=x)\n",
    "            emb = enc.pooler_output.detach().cpu().numpy()\n",
    "            Xs.append(emb)\n",
    "            ys.append(y.numpy())\n",
    "            fns.extend(fn)\n",
    "            seen += emb.shape[0]\n",
    "            if max_items and seen >= max_items:\n",
    "                break\n",
    "    return np.concatenate(Xs), np.concatenate(ys), np.array(fns)\n",
    "\n",
    "def make_discrete_cmap(num_classes: int, seed: int = 0):\n",
    "    base = plt.get_cmap(\"tab20\").colors\n",
    "    colors = (list(base) * ((num_classes // len(base)) + 1))[:num_classes]\n",
    "    rng = np.random.default_rng(seed)\n",
    "    perm = rng.permutation(num_classes)\n",
    "    colors = [colors[i] for i in perm]\n",
    "    cmap = mpl.colors.ListedColormap(colors)\n",
    "    norm = mpl.colors.BoundaryNorm(np.arange(-0.5, num_classes + 0.5, 1), cmap.N)\n",
    "    return cmap, norm\n",
    "\n",
    "def plot_umap(Z2, y, title, df_meta):\n",
    "    num_classes = int(df_meta[\"target\"].nunique())\n",
    "    cmap, norm = make_discrete_cmap(num_classes, seed=int(cfg[\"project\"][\"seed\"]))\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sc = plt.scatter(Z2[:, 0], Z2[:, 1], c=y, s=8, alpha=0.55, cmap=cmap, norm=norm, linewidths=0)\n",
    "    cb = plt.colorbar(sc, ticks=np.arange(num_classes))\n",
    "    cb.set_label(\"target id\")\n",
    "    cb.ax.tick_params(labelsize=6)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"UMAP-1\")\n",
    "    plt.ylabel(\"UMAP-2\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "df_all = pd.concat([splits[\"train\"], splits[\"val\"], splits[\"test\"]], axis=0).reset_index(drop=True)\n",
    "ds_all = Esc50AstDataset(df_all, audio_dir, feature_extractor)\n",
    "all_loader = DataLoader(ds_all, batch_size=batch_size, shuffle=False, num_workers=num_workers,\n",
    "                        collate_fn=collate_ast_fixed)\n",
    "\n",
    "Xin, y_all, fn_all = batch_input_features(all_loader)\n",
    "Xenc, _, _         = batch_encoded_features(model, all_loader, device)\n",
    "\n",
    "um_in  = umap.UMAP(n_neighbors=20, min_dist=0.15, metric=\"euclidean\", random_state=int(cfg[\"project\"][\"seed\"]))\n",
    "Z_in   = um_in.fit_transform(Xin)\n",
    "\n",
    "um_enc = umap.UMAP(n_neighbors=20, min_dist=0.15, metric=\"euclidean\", random_state=int(cfg[\"project\"][\"seed\"]))\n",
    "Z_enc  = um_enc.fit_transform(Xenc)\n",
    "\n",
    "plot_umap(Z_in,  y_all, \"UMAP: AST input features (mean+std over time per mel)\", df)\n",
    "plot_umap(Z_enc, y_all, \"UMAP: AST encoded features (pooler_output)\", df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) 指定ラベルだけ可視化（ラベル名リストで選択）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_umap_selected(Z2, y, df_meta, labels=None, emphasize=True, title=\"UMAP selected\"):\n",
    "    t2c = df_meta.sort_values(\"target\").drop_duplicates(\"target\").set_index(\"target\")[\"category\"].to_dict()\n",
    "    c2t = {c: int(t) for t, c in t2c.items()}\n",
    "    num_classes = int(df_meta[\"target\"].nunique())\n",
    "\n",
    "    if labels is None:\n",
    "        sel_targets = np.arange(num_classes, dtype=int)\n",
    "        labels_txt = \"ALL\"\n",
    "    else:\n",
    "        missing = [c for c in labels if c not in c2t]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Unknown category: {missing}\")\n",
    "        sel_targets = np.array([c2t[c] for c in labels], dtype=int)\n",
    "        labels_txt = \", \".join(labels)\n",
    "\n",
    "    sel_mask = np.isin(y, sel_targets)\n",
    "    K = len(sel_targets)\n",
    "\n",
    "    base = plt.get_cmap(\"tab20\").colors\n",
    "    colors = (list(base) * ((K // len(base)) + 1))[:K]\n",
    "    rng = np.random.default_rng(int(cfg[\"project\"][\"seed\"]))\n",
    "    perm = rng.permutation(K)\n",
    "    colors = [colors[i] for i in perm]\n",
    "    cmap = mpl.colors.ListedColormap(colors)\n",
    "\n",
    "    target_to_index = {int(t): i for i, t in enumerate(sel_targets.tolist())}\n",
    "    c_sel = np.array([target_to_index[int(t)] for t in y[sel_mask]], dtype=int)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    if emphasize and labels is not None:\n",
    "        plt.scatter(Z2[:, 0], Z2[:, 1], c=\"lightgray\", s=8, alpha=0.12, linewidths=0)\n",
    "\n",
    "    sc = plt.scatter(Z2[sel_mask, 0], Z2[sel_mask, 1], c=c_sel, s=10, alpha=0.65, cmap=cmap, linewidths=0)\n",
    "\n",
    "    cb = plt.colorbar(sc, ticks=np.arange(K))\n",
    "    cb.ax.set_yticklabels([t2c[int(t)] for t in sel_targets])\n",
    "    cb.set_label(\"category\")\n",
    "    cb.ax.tick_params(labelsize=8)\n",
    "\n",
    "    plt.title(f\"{title} | labels={labels_txt}\")\n",
    "    plt.xlabel(\"UMAP-1\")\n",
    "    plt.ylabel(\"UMAP-2\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 例:\n",
    "# plot_umap_selected(Z_in,  y_all, df, labels=[\"dog\",\"rain\",\"chainsaw\"], title=\"UMAP input features\")\n",
    "# plot_umap_selected(Z_enc, y_all, df, labels=[\"dog\",\"rain\",\"chainsaw\"], title=\"UMAP encoded features\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
